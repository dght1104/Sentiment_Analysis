{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "from textblob import TextBlob\n",
    "from googletrans import Translator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional, LSTM, GRU, Input, GlobalMaxPooling1D, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pyvi import ViTokenizer, ViUtils\n",
    "from vncorenlp import VnCoreNLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in ra 5 mẫu dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>mtime</th>\n",
       "      <th>rating_star</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69545822</td>\n",
       "      <td>20/05/2021 11:29</td>\n",
       "      <td>5</td>\n",
       "      <td>Đồ giống hình, nhìn khá xinh, giá rẻ, ship siê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49021104</td>\n",
       "      <td>08/05/2021 22:10</td>\n",
       "      <td>5</td>\n",
       "      <td>Giao hàng nhanh. Mình đặt hộ nên ko biết chất ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>275113953</td>\n",
       "      <td>30/06/2021 21:42</td>\n",
       "      <td>5</td>\n",
       "      <td>Sản phẩm giống trên hình. Son khá lì. Thời gia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>470905623</td>\n",
       "      <td>30/06/2021 21:42</td>\n",
       "      <td>5</td>\n",
       "      <td>Son xinh xĩu lun á shopp ơii.vỏ son nhìn cute ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321034368</td>\n",
       "      <td>19/05/2021 12:08</td>\n",
       "      <td>5</td>\n",
       "      <td>Cách xoá MỤN ĐẦU ĐEN mũi bằng DẦU TẨY TRANG. K...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userid             mtime  rating_star  \\\n",
       "0   69545822  20/05/2021 11:29            5   \n",
       "1   49021104  08/05/2021 22:10            5   \n",
       "2  275113953  30/06/2021 21:42            5   \n",
       "3  470905623  30/06/2021 21:42            5   \n",
       "4  321034368  19/05/2021 12:08            5   \n",
       "\n",
       "                                             comment  \n",
       "0  Đồ giống hình, nhìn khá xinh, giá rẻ, ship siê...  \n",
       "1  Giao hàng nhanh. Mình đặt hộ nên ko biết chất ...  \n",
       "2  Sản phẩm giống trên hình. Son khá lì. Thời gia...  \n",
       "3  Son xinh xĩu lun á shopp ơii.vỏ son nhìn cute ...  \n",
       "4  Cách xoá MỤN ĐẦU ĐEN mũi bằng DẦU TẨY TRANG. K...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"sentiments_v2_1_1000.csv\")\n",
    "comments = data['comment'].tolist()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chuẩn hóa và loại bỏ trùng lắp dl (*Case: DAU TAY TRANG, có thể hiểu 2 nghĩa trong tiếng Việt 'Dầu tẩy trang' / 'Dâu tây trắng' => Làm sao để phân biệt được?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 là số lượng bình luận sau khi chuẩn hóa và loại bỏ trùng lặp.\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "unique_comments = list(set(comments))\n",
    "\n",
    "def remove_vietnamese_accents(text):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "normalized_comments = list(set([remove_vietnamese_accents(comment).lower() for comment in comments]))\n",
    "print(len(normalized_comments), \"là số lượng bình luận sau khi chuẩn hóa và loại bỏ trùng lặp.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER Sentiment Analysis + googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cảm xúc: Trung lập, Compound Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "# nhập từ bàn phím\n",
    "translator = Translator()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Hàm phân tích cảm xúc cho câu tiếng Việt và trả về cả nhãn và giá trị compound\n",
    "def classify_sentiment_vietnamese(comment):\n",
    "    # Dịch câu sang tiếng Anh\n",
    "    translated_text = translator.translate(comment, src='vi', dest='en').text\n",
    "    # Phân tích cảm xúc của câu đã dịch\n",
    "    sentiment_score = analyzer.polarity_scores(translated_text)\n",
    "    compound_score = sentiment_score['compound'] #Compound này lấy từ cái gần tọa độ \n",
    "    \n",
    "    # Phân loại cảm xúc dựa trên giá trị compound\n",
    "    if compound_score > 0:\n",
    "        sentiment = 'Tích cực'\n",
    "    elif compound_score < 0:\n",
    "        sentiment = 'Tiêu cực'\n",
    "    else:\n",
    "        sentiment = 'Trung lập'\n",
    "    \n",
    "    # Trả về nhãn cảm xúc và giá trị compound\n",
    "    return sentiment, compound_score\n",
    "\n",
    "# Ví dụ câu tiếng Việt\n",
    "comment = \"Không có có\"\n",
    "sentiment, compound_score = classify_sentiment_vietnamese(comment)\n",
    "print(f\"Cảm xúc: {sentiment}, Compound Score: {compound_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      userid             mtime  rating_star  \\\n",
      "0   69545822  20/05/2021 11:29            5   \n",
      "1   49021104  08/05/2021 22:10            5   \n",
      "2  275113953  30/06/2021 21:42            5   \n",
      "3  470905623  30/06/2021 21:42            5   \n",
      "4  321034368  19/05/2021 12:08            5   \n",
      "5   23364385  26/05/2021 15:04            5   \n",
      "6  124689928  05/06/2021 13:52            5   \n",
      "7   70572173  30/06/2021 12:58            5   \n",
      "8   58074290  14/05/2021 16:57            5   \n",
      "9  354970721  16/05/2021 10:21            5   \n",
      "\n",
      "                                             comment  Sentiment  \\\n",
      "0  Đồ giống hình, nhìn khá xinh, giá rẻ, ship siê...   Tích cực   \n",
      "1  Giao hàng nhanh. Mình đặt hộ nên ko biết chất ...   Tích cực   \n",
      "2  Sản phẩm giống trên hình. Son khá lì. Thời gia...  Trung lập   \n",
      "3  Son xinh xĩu lun á shopp ơii.vỏ son nhìn cute ...   Tích cực   \n",
      "4  Cách xoá MỤN ĐẦU ĐEN mũi bằng DẦU TẨY TRANG. K...  Trung lập   \n",
      "5  Xinh lắm các bạn ơi bling bling lắm giao hàng ...   Tích cực   \n",
      "6  shop giao hàng đủ, giao hàng nhanh, sản phẩm o...  Trung lập   \n",
      "7  son trông khá cute, test thử lên tay màu cũng ...   Tích cực   \n",
      "8  Chất son ok phù hợp giá tiền gói hàng nhanh gi...   Tích cực   \n",
      "9         Anbxkdkdkdmdndnxk d k ksks bsskskansnsiakk  Trung lập   \n",
      "\n",
      "   Compound Score  \n",
      "0          0.8625  \n",
      "1          0.8977  \n",
      "2          0.0000  \n",
      "3          0.5994  \n",
      "4          0.0000  \n",
      "5          0.8610  \n",
      "6          0.0000  \n",
      "7          0.7574  \n",
      "8          0.4466  \n",
      "9          0.0000  \n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "translator = Translator()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Hàm phân tích cảm xúc cho câu tiếng Việt và trả về cả nhãn và giá trị compound\n",
    "def classify_sentiment_vietnamese(comment):\n",
    "    # Dịch câu sang tiếng Anh\n",
    "    translated_text = translator.translate(comment, src='vi', dest='en').text  # Sử dụng .text để lấy văn bản đã dịch\n",
    "    # Phân tích cảm xúc của câu đã dịch\n",
    "    sentiment_score = analyzer.polarity_scores(translated_text)\n",
    "    compound_score = sentiment_score['compound']\n",
    "    \n",
    "    # Phân loại cảm xúc dựa trên giá trị compound\n",
    "    if compound_score > 0:\n",
    "        sentiment = 'Tích cực'\n",
    "    elif compound_score < 0:\n",
    "        sentiment = 'Tiêu cực'\n",
    "    else:\n",
    "        sentiment = 'Trung lập'\n",
    "    \n",
    "    return sentiment, compound_score\n",
    "\n",
    "# Sử dụng hàm để phân tích cảm xúc cho các bình luận\n",
    "results = [classify_sentiment_vietnamese(comment) for comment in comments]\n",
    "\n",
    "# Chuyển kết quả phân tích thành DataFrame\n",
    "sentiments_df = pd.DataFrame(results, columns=['Sentiment', 'Compound Score'])\n",
    "\n",
    "# Thêm các kết quả vào dữ liệu gốc\n",
    "data['Sentiment'] = sentiments_df['Sentiment']\n",
    "data['Compound Score'] = sentiments_df['Compound Score']\n",
    "\n",
    "# In ra 10 dòng đầu tiên với kết quả phân tích\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VnCoreNLP + VADER + TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      userid             mtime  rating_star  \\\n",
      "0   69545822  20/05/2021 11:29            5   \n",
      "1   49021104  08/05/2021 22:10            5   \n",
      "2  275113953  30/06/2021 21:42            5   \n",
      "3  470905623  30/06/2021 21:42            5   \n",
      "4  321034368  19/05/2021 12:08            5   \n",
      "5   23364385  26/05/2021 15:04            5   \n",
      "6  124689928  05/06/2021 13:52            5   \n",
      "7   70572173  30/06/2021 12:58            5   \n",
      "8   58074290  14/05/2021 16:57            5   \n",
      "9  354970721  16/05/2021 10:21            5   \n",
      "\n",
      "                                             comment  Sentiment  \\\n",
      "0  Đồ giống hình, nhìn khá xinh, giá rẻ, ship siê...   Tích cực   \n",
      "1  Giao hàng nhanh. Mình đặt hộ nên ko biết chất ...   Tích cực   \n",
      "2  Sản phẩm giống trên hình. Son khá lì. Thời gia...  Trung lập   \n",
      "3  Son xinh xĩu lun á shopp ơii.vỏ son nhìn cute ...   Tích cực   \n",
      "4  Cách xoá MỤN ĐẦU ĐEN mũi bằng DẦU TẨY TRANG. K...  Trung lập   \n",
      "5  Xinh lắm các bạn ơi bling bling lắm giao hàng ...   Tích cực   \n",
      "6  shop giao hàng đủ, giao hàng nhanh, sản phẩm o...  Trung lập   \n",
      "7  son trông khá cute, test thử lên tay màu cũng ...   Tích cực   \n",
      "8  Chất son ok phù hợp giá tiền gói hàng nhanh gi...   Tích cực   \n",
      "9         Anbxkdkdkdmdndnxk d k ksks bsskskansnsiakk  Trung lập   \n",
      "\n",
      "   Compound Score  \n",
      "0          0.8625  \n",
      "1          0.8977  \n",
      "2          0.0000  \n",
      "3          0.5994  \n",
      "4          0.0000  \n",
      "5          0.8610  \n",
      "6          0.0000  \n",
      "7          0.7574  \n",
      "8          0.4466  \n",
      "9          0.0000  \n"
     ]
    }
   ],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "VnCoreNLP_PATH = \"E:/nam4/hk1/doan/VnCoreNLP/VnCoreNLP-1.1.1.jar\"\n",
    "annotator = VnCoreNLP(VnCoreNLP_PATH, annotators=\"wseg,pos\", max_heap_size='-Xmx2g')\n",
    "\n",
    "# Khởi tạo VADER và Translator\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "translator = Translator()\n",
    "\n",
    "# Hàm dịch văn bản từ tiếng Việt sang tiếng Anh\n",
    "def translate_to_english(text):\n",
    "    try:\n",
    "        return translator.translate(text, src='vi', dest='en').text\n",
    "    except Exception as e:\n",
    "        return text  # Nếu lỗi, trả lại văn bản gốc\n",
    "\n",
    "# Hàm phân tích cảm xúc sử dụng VADER và TextBlob\n",
    "def analyze_sentiment_vietnamese(comment):\n",
    "    # Bước 1: Dịch văn bản từ tiếng Việt sang tiếng Anh\n",
    "    translated_comment = translate_to_english(comment)\n",
    "    \n",
    "    # Bước 2: Sử dụng VADER để phân tích cảm xúc\n",
    "    sentiment_score_vader = analyzer.polarity_scores(translated_comment)\n",
    "    sentiment_score_blob = TextBlob(translated_comment).sentiment.polarity\n",
    "\n",
    "    # Bước 3: Phân tích cảm xúc từ VADER\n",
    "    sentiment_vader = sentiment_score_vader['compound']\n",
    "    \n",
    "    # Quyết định cảm xúc từ VADER\n",
    "    if sentiment_vader > 0.1:\n",
    "        sentiment = \"Tích cực\"\n",
    "        score = sentiment_vader\n",
    "    elif sentiment_vader < -0.1:\n",
    "        sentiment = \"Tiêu cực\"\n",
    "        score = sentiment_vader\n",
    "    else:\n",
    "        sentiment = \"Trung lập\"\n",
    "        score = 0\n",
    "    \n",
    "    return sentiment, score\n",
    "\n",
    "# Phân tích cảm xúc cho tất cả các bình luận\n",
    "results = [analyze_sentiment_vietnamese(comment) for comment in comments]\n",
    "\n",
    "# Chuyển kết quả phân tích thành DataFrame\n",
    "sentiments_df = pd.DataFrame(results, columns=['Sentiment', 'Compound Score'])\n",
    "\n",
    "# Thêm các kết quả vào dữ liệu gốc để xem bình luận kèm nhãn cảm xúc\n",
    "data['Sentiment'] = sentiments_df['Sentiment']\n",
    "data['Compound Score'] = sentiments_df['Compound Score']\n",
    "\n",
    "# In ra 10 dòng đầu tiên với kết quả phân tích\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (PhoBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load tokenizer và model PhoBERT cho tiếng Việt\n",
    "tokenizer = BertTokenizer.from_pretrained('vinai/phobert-base')\n",
    "model = TFBertForSequenceClassification.from_pretrained('vinai/phobert-base')\n",
    "\n",
    "# Tiền xử lý văn bản tiếng Việt\n",
    "text = \"hôm nay xe của người tôi ghét bị ngập nước \"\n",
    "inputs = tokenizer(text, return_tensors='tf', truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Dự đoán cảm xúc\n",
    "outputs = model(inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class = tf.argmax(logits, axis=-1).numpy()\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n",
    "\n",
    "if predicted_class == 1:\n",
    "    print(\"tích cực\")\n",
    "else:\n",
    "    print (\"tiêu cực\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      userid             mtime  rating_star  \\\n",
      "0   69545822  20/05/2021 11:29            5   \n",
      "1   49021104  08/05/2021 22:10            5   \n",
      "2  275113953  30/06/2021 21:42            5   \n",
      "3  470905623  30/06/2021 21:42            5   \n",
      "4  321034368  19/05/2021 12:08            5   \n",
      "5   23364385  26/05/2021 15:04            5   \n",
      "6  124689928  05/06/2021 13:52            5   \n",
      "7   70572173  30/06/2021 12:58            5   \n",
      "8   58074290  14/05/2021 16:57            5   \n",
      "9  354970721  16/05/2021 10:21            5   \n",
      "\n",
      "                                             comment  Sentiment  \\\n",
      "0  Đồ giống hình, nhìn khá xinh, giá rẻ, ship siê...   Tích cực   \n",
      "1  Giao hàng nhanh. Mình đặt hộ nên ko biết chất ...  Trung lập   \n",
      "2  Sản phẩm giống trên hình. Son khá lì. Thời gia...  Trung lập   \n",
      "3  Son xinh xĩu lun á shopp ơii.vỏ son nhìn cute ...  Trung lập   \n",
      "4  Cách xoá MỤN ĐẦU ĐEN mũi bằng DẦU TẨY TRANG. K...  Trung lập   \n",
      "5  Xinh lắm các bạn ơi bling bling lắm giao hàng ...   Tích cực   \n",
      "6  shop giao hàng đủ, giao hàng nhanh, sản phẩm o...  Trung lập   \n",
      "7  son trông khá cute, test thử lên tay màu cũng ...  Trung lập   \n",
      "8  Chất son ok phù hợp giá tiền gói hàng nhanh gi...  Trung lập   \n",
      "9         Anbxkdkdkdmdndnxk d k ksks bsskskansnsiakk   Tiêu cực   \n",
      "\n",
      "   Compound Score  \n",
      "0               1  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n",
      "5               1  \n",
      "6               0  \n",
      "7               0  \n",
      "8               0  \n",
      "9              -1  \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Khởi tạo mô hình phân tích cảm xúc sử dụng XLM-R (mô hình BERT đa ngôn ngữ)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Hàm phân tích cảm xúc cho câu tiếng Việt mà không cần liệt kê từ khóa\n",
    "def classify_sentiment_vietnamese(comment):\n",
    "    # Sử dụng mô hình đã huấn luyện để phân tích cảm xúc\n",
    "    result = classifier(comment)\n",
    "    \n",
    "    # Trả về nhãn cảm xúc và giá trị cảm xúc\n",
    "    label = result[0]['label']\n",
    "    score = result[0]['score']\n",
    "    \n",
    "    # Phân loại cảm xúc theo nhãn của mô hình\n",
    "    if label == '5 stars':\n",
    "        sentiment = 'Tích cực'\n",
    "        compound_score = 1\n",
    "    elif label == '1 star':\n",
    "        sentiment = 'Tiêu cực'\n",
    "        compound_score = -1\n",
    "    else:\n",
    "        sentiment = 'Trung lập'\n",
    "        compound_score = 0\n",
    "    \n",
    "    return sentiment, compound_score\n",
    "\n",
    "# Đọc dữ liệu từ file CSV\n",
    "data = pd.read_csv(\"sentiments_v2_1_1000.csv\")\n",
    "\n",
    "# Lấy danh sách các bình luận\n",
    "comments = data['comment'].tolist()\n",
    "\n",
    "# Phân tích cảm xúc cho tất cả các bình luận\n",
    "results = [classify_sentiment_vietnamese(comment) for comment in comments]\n",
    "\n",
    "# Chuyển kết quả phân tích thành DataFrame\n",
    "sentiments_df = pd.DataFrame(results, columns=['Sentiment', 'Compound Score'])\n",
    "\n",
    "# Thêm các kết quả vào dữ liệu gốc để xem bình luận kèm nhãn cảm xúc\n",
    "data['Sentiment'] = sentiments_df['Sentiment']\n",
    "data['Compound Score'] = sentiments_df['Compound Score']\n",
    "\n",
    "# In ra 10 dòng đầu tiên với kết quả phân tích\n",
    "print(data.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userid', 'mtime', 'rating_star', 'comment', 'sentiment',\n",
      "       'predicted_sentiment'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 497ms/step - accuracy: 0.8966 - loss: 0.5530 - val_accuracy: 1.0000 - val_loss: 0.0204\n",
      "Epoch 2/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 348ms/step - accuracy: 0.9997 - loss: 0.0156 - val_accuracy: 1.0000 - val_loss: 0.0021\n",
      "Epoch 3/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 317ms/step - accuracy: 0.9994 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 4/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 295ms/step - accuracy: 0.9964 - loss: 0.0258 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
      "Epoch 5/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 309ms/step - accuracy: 0.9997 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 9.3796e-04\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step\n",
      "                                               comment  sentiment  \\\n",
      "0    Đồ giống hình, nhìn khá xinh, giá rẻ, ship siê...          0   \n",
      "1    Giao hàng nhanh. Mình đặt hộ nên ko biết chất ...          0   \n",
      "2    Sản phẩm giống trên hình. Son khá lì. Thời gia...          0   \n",
      "3    Son xinh xĩu lun á shopp ơii.vỏ son nhìn cute ...          0   \n",
      "4    Cách xoá MỤN ĐẦU ĐEN mũi bằng DẦU TẨY TRANG. K...          0   \n",
      "..                                                 ...        ...   \n",
      "994  Mình thấy son lên màu k được giống ảnh. Vỏ ngo...          0   \n",
      "995                                 Đồ ăn ngon lắm nhé          0   \n",
      "996                                  . Son mịn màu đẹp          0   \n",
      "997  Son đẹp giao hàng nhanhh.Màu cực ưng luôn á.Đá...          0   \n",
      "998      Xinh khủng còn được tặng dây cột tóc nữa chứ.          0   \n",
      "\n",
      "     predicted_sentiment  \n",
      "0                      0  \n",
      "1                      0  \n",
      "2                      0  \n",
      "3                      0  \n",
      "4                      0  \n",
      "..                   ...  \n",
      "994                    0  \n",
      "995                    0  \n",
      "996                    0  \n",
      "997                    0  \n",
      "998                    0  \n",
      "\n",
      "[999 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# In ra tên các cột để chắc chắn rằng cột 'comment' và 'sentiment' tồn tại\n",
    "print(data.columns)\n",
    "\n",
    "# Nếu không có cột 'sentiment', tạo nhãn cảm xúc\n",
    "if 'sentiment' not in data.columns:\n",
    "    def label_sentiment(comment):\n",
    "        if 'good' in comment or 'love' in comment:\n",
    "            return 1  # Tích cực\n",
    "        else:\n",
    "            return 0  # Tiêu cực\n",
    "    data['sentiment'] = data['comment'].apply(label_sentiment)\n",
    "\n",
    "# 2. Tiền xử lý dữ liệu\n",
    "max_features = 10000  # Giới hạn số từ để giữ lại\n",
    "max_len = 200  # Độ dài tối đa của mỗi comment\n",
    "\n",
    "# Tokenizer để chuyển văn bản thành các chỉ số\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(data['comment'])  # Tạo từ điển các từ trong dữ liệu (cột 'comment')\n",
    "x_data = tokenizer.texts_to_sequences(data['comment'])  # Chuyển đổi văn bản thành chỉ số\n",
    "x_data = pad_sequences(x_data, maxlen=max_len)  # Padding cho các câu có độ dài khác nhau\n",
    "\n",
    "# 3. Mã hóa nhãn cảm xúc\n",
    "y_data = data['sentiment'].values  # Nhãn cảm xúc (1 cho tích cực, 0 cho tiêu cực)\n",
    "\n",
    "# 4. Xây dựng mô hình phân loại cảm xúc\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_features, output_dim=128, input_length=max_len),\n",
    "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 5. Compile mô hình\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 6. Huấn luyện mô hình\n",
    "model.fit(x_data, y_data, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# 7. Dự đoán cảm xúc cho tất cả các comment trong file CSV\n",
    "predictions = model.predict(x_data)\n",
    "\n",
    "# Chuyển đổi dự đoán thành nhãn (1 là tích cực, 0 là tiêu cực)\n",
    "predicted_labels = [1 if pred >= 0.5 else 0 for pred in predictions]\n",
    "\n",
    "# Thêm kết quả dự đoán vào DataFrame\n",
    "data['predicted_sentiment'] = predicted_labels\n",
    "\n",
    "# 8. Lưu kết quả vào file CSV mới\n",
    "data.to_csv('reviews_with_sentiment_predictions.csv', index=False)\n",
    "\n",
    "# 9. Hiển thị kết quả\n",
    "print(data[['comment', 'sentiment', 'predicted_sentiment']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userid', 'mtime', 'rating_star', 'comment', 'tokenized_comment',\n",
      "       'cleaned_comment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Đọc file CSV\n",
    "file_path = \"sentiments_v2_7001_7230.csv\"\n",
    "data = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "\n",
    "# In ra tên các cột để kiểm tra\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dụng thư viện textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các cột trong DataFrame: Index(['userid', 'mtime', 'rating_star', 'comment'], dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment  sentiment\n",
      "0  Đồ giống hình, nhìn khá xinh, giá rẻ, ship siê...   0.295833\n",
      "1  Giao hàng nhanh. Mình đặt hộ nên ko biết chất ...   0.391667\n",
      "2  Sản phẩm giống trên hình. Son khá lì. Thời gia...  -0.025000\n",
      "3  Son xinh xĩu lun á shopp ơii.vỏ son nhìn cute ...   0.850000\n",
      "4  Cách xoá MỤN ĐẦU ĐEN mũi bằng DẦU TẨY TRANG. K...  -0.125000\n"
     ]
    }
   ],
   "source": [
    "# In ra tên các cột để kiểm tra\n",
    "print(\"Các cột trong DataFrame:\", data.columns)\n",
    "\n",
    "# Chỉ kiểm tra nếu cột 'comment' tồn tại\n",
    "if 'comment' in data.columns:\n",
    "    # Khởi tạo công cụ dịch\n",
    "    translator = Translator()\n",
    "\n",
    "    # Hàm dịch và phân tích cảm xúc\n",
    "    def analyze_sentiment(text):\n",
    "        # Dịch văn bản sang tiếng Anh\n",
    "        translated_text = translator.translate(text, src='vi', dest='en').text\n",
    "        # Phân tích cảm xúc bằng TextBlob\n",
    "        blob = TextBlob(translated_text)\n",
    "        return blob.sentiment.polarity  # Trả về điểm cảm xúc từ -1 (tiêu cực) đến 1 (tích cực)\n",
    "\n",
    "    # Áp dụng hàm cho 5 bình luận đầu tiên\n",
    "    data['sentiment'] = data['comment'].head(5).apply(analyze_sentiment)\n",
    "\n",
    "    # Hiển thị 5 bình luận đầu tiên với điểm cảm xúc\n",
    "    print(data[['comment', 'sentiment']].head(5))\n",
    "else:\n",
    "    print(\"Cột 'comment' không tồn tại trong DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các cột trong DataFrame: Index(['userid', 'mtime', 'rating_star', 'comment', 'tokenized_comment',\n",
      "       'cleaned_comment'],\n",
      "      dtype='object')\n",
      "                                             comment  \\\n",
      "0             Giá rẻ. Mua xịt khử mùi ăn uóng khá ổn   \n",
      "1  thơm lắm mn oi có chai đào là khi xịt hòi là h...   \n",
      "2  eidighgghuđhuêd Dbddnd ĐĐ D D c chr ý nghĩa là...   \n",
      "3    Thơm nha các bạn nhưng k giữ đc mùi lâu đâu nhé   \n",
      "4                         Sản phẩm giống hình xài ok   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0            Giá rẻ . Mua xịt khử mùi ăn uóng khá ổn  \n",
      "1  thơm lắm mn oi có chai đào là khi xịt hòi là h...  \n",
      "2  eidighgghuđhuêd Dbddnd ĐĐ D D c chr ý_nghĩa_là...  \n",
      "3    Thơm nha các bạn nhưng k giữ đc mùi lâu đâu nhé  \n",
      "4                         Sản_phẩm giống hình xài ok  \n"
     ]
    }
   ],
   "source": [
    "# Đọc file CSV và bỏ qua các dòng lỗi\n",
    "file_path = \"sentiments_v2_7001_7230.csv\"\n",
    "data = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "\n",
    "# In ra tên các cột để kiểm tra\n",
    "print(\"Các cột trong DataFrame:\", data.columns)\n",
    "\n",
    "# Giả sử cột chứa văn bản là 'text' (thay bằng tên cột tương ứng nếu khác)\n",
    "if 'comment' in data.columns:\n",
    "    # Tách từ cho từng dòng trong cột 'text' và lưu vào một cột mới 'tokenized_text'\n",
    "    data['tokenized_text'] = data['comment'].apply(ViTokenizer.tokenize)\n",
    "\n",
    "    output_file_path = \"sentiments_tk_7001_7230.csv\"\n",
    "    data.to_csv(output_file_path, index=False)  # Không lưu chỉ mục (index)\n",
    "    \n",
    "    print(data[['comment', 'tokenized_text']].head())\n",
    "else:\n",
    "    print(\"Không tìm thấy cột 'comment' trong DataFrame. Vui lòng kiểm tra lại tên cột.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các cột trong DataFrame: Index(['userid', 'mtime', 'rating_star', 'comment', 'tokenized_comment',\n",
      "       'cleaned_comment'],\n",
      "      dtype='object')\n",
      "Dữ liệu đã được lưu vào file: sentiments_v2_7001_7230.csv\n"
     ]
    }
   ],
   "source": [
    "# Danh sách các stop words tiếng Việt (có thể mở rộng thêm)\n",
    "stop_words = set([\n",
    "    \"và\", \"của\", \"là\", \"đã\", \"đang\", \"nên\", \"nhưng\", \"với\", \"bằng\", \"theo\", \n",
    "    \"làm\", \"từ\", \"trong\", \"lúc\", \"mà\", \"này\", \"có\", \"chưa\", \"vì\", \"sẽ\", \"khi\",\n",
    "    \"được\", \"không\", \"tôi\", \"bạn\", \"chúng\", \"mình\", \"một\", \"nhiều\", \"vài\", \n",
    "    \"cũng\", \"nào\", \"nếu\", \"hay\", \"kể\", \"vừa\", \"nên\", \"có thể\", \"làm\", \"mà\"\n",
    "])\n",
    "\n",
    "# In ra tên các cột để kiểm tra\n",
    "print(\"Các cột trong DataFrame:\", data.columns)\n",
    "\n",
    "# Giả sử cột chứa văn bản là 'comment' (thay bằng tên cột khác nếu cần)\n",
    "if 'comment' in data.columns:\n",
    "    # Tách từ trong cột 'comment'\n",
    "    data['tokenized_comment'] = data['comment'].apply(ViTokenizer.tokenize)\n",
    "    \n",
    "    # Bước loại bỏ stop words\n",
    "    def remove_stopwords(text):\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        return \" \".join(filtered_words)\n",
    "    \n",
    "    # Áp dụng loại bỏ stop words\n",
    "    data['cleaned_comment'] = data['tokenized_comment'].apply(remove_stopwords)\n",
    "\n",
    "    # Lưu kết quả vào file CSV mới\n",
    "    output_file_path = \"sentiments_v2_7001_7230.csv\"\n",
    "    data.to_csv(output_file_path, index=False)  # Không lưu chỉ mục (index)\n",
    "\n",
    "    print(f\"Dữ liệu đã được lưu vào file: {output_file_path}\")\n",
    "else:\n",
    "    print(\"Không tìm thấy cột 'comment' trong DataFrame. Vui lòng kiểm tra lại tên cột.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VnCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematization đã hoàn thành và lưu vào file lemmatized_sentiments.csv\n"
     ]
    }
   ],
   "source": [
    "# Đặt đường dẫn tới mô hình VnCoreNLP\n",
    "vncorenlp_path = r'D:\\Y4 HK1\\VnCoreNLP\\VnCoreNLP-1.1.1.jar'  # Thay bằng đường dẫn chính xác của bạn\n",
    "\n",
    "# Khởi tạo VnCoreNLP\n",
    "vncorenlp = VnCoreNLP(vncorenlp_path)\n",
    "\n",
    "if 'comment' in data.columns:\n",
    "    # Áp dụng lemmatization cho mỗi dòng trong cột 'comment'\n",
    "    def lemmatize_text(text):\n",
    "    # Dùng VnCoreNLP để annotate văn bản\n",
    "        annotated = vncorenlp.annotate(text)\n",
    "        \n",
    "        # Trích xuất các từ đã được lemmatize từ dữ liệu trả về\n",
    "        words = []\n",
    "        for sentence in annotated.get('sentences', []):\n",
    "            for token in sentence:\n",
    "                # Kiểm tra nếu 'lemma' tồn tại trong token\n",
    "                if 'lemma' in token:\n",
    "                    words.append(token['lemma'])\n",
    "                else:\n",
    "                    words.append(token['form'])  # Nếu không có 'lemma', lấy từ gốc\n",
    "        \n",
    "        # Trả về các từ đã lemmatized\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Áp dụng lemmatization cho mỗi dòng trong cột 'comment'\n",
    "    data['lemmatized_comment'] = data['comment'].apply(lemmatize_text)\n",
    "\n",
    "    # Lưu lại kết quả vào file CSV mới\n",
    "    data.to_csv('lemmatized_sentiments.csv', index=False)  # Lưu vào file mới\n",
    "\n",
    "    print(\"Lematization đã hoàn thành và lưu vào file lemmatized_sentiments.csv\")\n",
    "else:\n",
    "    print(\"Không tìm thấy cột 'comment' trong DataFrame.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
