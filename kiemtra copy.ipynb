{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "from textblob import TextBlob\n",
    "from googletrans import Translator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional, LSTM, GRU, Input, GlobalMaxPooling1D, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pyvi import ViTokenizer, ViUtils\n",
    "from vncorenlp import VnCoreNLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userid', 'mtime', 'rating_star', 'comment'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.8959 - loss: 0.5870 - val_accuracy: 1.0000 - val_loss: 0.0456\n",
      "Epoch 2/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9918 - loss: 0.0588 - val_accuracy: 1.0000 - val_loss: 0.0035\n",
      "Epoch 3/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.9959 - loss: 0.0267 - val_accuracy: 1.0000 - val_loss: 0.0021\n",
      "Epoch 4/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9968 - loss: 0.0212 - val_accuracy: 1.0000 - val_loss: 0.0030\n",
      "Epoch 5/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9957 - loss: 0.0278 - val_accuracy: 1.0000 - val_loss: 0.0046\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "                                               comment  sentiment  \\\n",
      "0    Dhshigdgiggyhudauoegvngsyhgxhgahbchkđghdgvxgbv...          0   \n",
      "1              drkdrkcnkddnshdkgosqnandcxnsgogkssndkco          0   \n",
      "2    Son môi rẻ, chất lượng oki nhưng không thích m...          0   \n",
      "3         Giao đầy đủ và chất lượng đáng mua nha mnnmm          0   \n",
      "4    Shop chuẩn bị hàng nhanh, đóng gói cẩn thận kĩ...          0   \n",
      "..                                                 ...        ...   \n",
      "994  Mùi đào thơm dịu thích hợp đi học hay đi chơi ...          0   \n",
      "995                                          Rất thơm.          0   \n",
      "996  Mùi thơm nhẹ vừa ý mình. Giao hàng nhanh, được...          0   \n",
      "997  Mùi thơm đóng gói kĩ càng mag nhìn ưng nói chu...          0   \n",
      "998  Shop giao hàng nhanh. Hương đào nhẹ ko nồng ch...          0   \n",
      "\n",
      "     predicted_sentiment  \n",
      "0                      0  \n",
      "1                      0  \n",
      "2                      0  \n",
      "3                      0  \n",
      "4                      0  \n",
      "..                   ...  \n",
      "994                    0  \n",
      "995                    0  \n",
      "996                    0  \n",
      "997                    0  \n",
      "998                    0  \n",
      "\n",
      "[999 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# 1. Đọc dữ liệu từ CSV\n",
    "df = pd.read_csv('sentiments_v2_6001_7000.csv')\n",
    "\n",
    "# In ra tên các cột để chắc chắn rằng cột 'comment' và 'sentiment' tồn tại\n",
    "print(df.columns)\n",
    "\n",
    "# Nếu không có cột 'sentiment', tạo nhãn cảm xúc\n",
    "if 'sentiment' not in df.columns:\n",
    "    def label_sentiment(comment):\n",
    "        if 'good' in comment or 'love' in comment:\n",
    "            return 1  # Tích cực\n",
    "        else:\n",
    "            return 0  # Tiêu cực\n",
    "    df['sentiment'] = df['comment'].apply(label_sentiment)\n",
    "\n",
    "# 2. Tiền xử lý dữ liệu\n",
    "max_features = 10000  # Giới hạn số từ để giữ lại\n",
    "max_len = 200  # Độ dài tối đa của mỗi comment\n",
    "\n",
    "# Tokenizer để chuyển văn bản thành các chỉ số\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df['comment'])  # Tạo từ điển các từ trong dữ liệu (cột 'comment')\n",
    "x_data = tokenizer.texts_to_sequences(df['comment'])  # Chuyển đổi văn bản thành chỉ số\n",
    "x_data = pad_sequences(x_data, maxlen=max_len)  # Padding cho các câu có độ dài khác nhau\n",
    "\n",
    "# 3. Mã hóa nhãn cảm xúc\n",
    "y_data = df['sentiment'].values  # Nhãn cảm xúc (1 cho tích cực, 0 cho tiêu cực)\n",
    "\n",
    "# 4. Xây dựng mô hình phân loại cảm xúc\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_features, output_dim=128, input_length=max_len),\n",
    "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 5. Compile mô hình\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 6. Huấn luyện mô hình\n",
    "model.fit(x_data, y_data, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# 7. Dự đoán cảm xúc cho tất cả các comment trong file CSV\n",
    "predictions = model.predict(x_data)\n",
    "\n",
    "# Chuyển đổi dự đoán thành nhãn (1 là tích cực, 0 là tiêu cực)\n",
    "predicted_labels = [1 if pred >= 0.5 else 0 for pred in predictions]\n",
    "\n",
    "# Thêm kết quả dự đoán vào DataFrame\n",
    "df['predicted_sentiment'] = predicted_labels\n",
    "\n",
    "# 8. Lưu kết quả vào file CSV mới\n",
    "df.to_csv('reviews_with_sentiment_predictions.csv', index=False)\n",
    "\n",
    "# 9. Hiển thị kết quả\n",
    "print(df[['comment', 'sentiment', 'predicted_sentiment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PhobertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFBertForSequenceClassification: ['roberta', 'lm_head']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['bert', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [0]\n",
      "tiêu cực\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load tokenizer và model PhoBERT cho tiếng Việt\n",
    "tokenizer = BertTokenizer.from_pretrained('vinai/phobert-base')\n",
    "model = TFBertForSequenceClassification.from_pretrained('vinai/phobert-base')\n",
    "\n",
    "# Tiền xử lý văn bản tiếng Việt\n",
    "text = \"sản phẩm này tốt \"\n",
    "inputs = tokenizer(text, return_tensors='tf', truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Dự đoán cảm xúc\n",
    "outputs = model(inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class = tf.argmax(logits, axis=-1).numpy()\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n",
    "\n",
    "if predicted_class == 1:\n",
    "    print(\"tích cực\")\n",
    "else:\n",
    "    print (\"tiêu cực\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userid', 'mtime', 'rating_star', 'comment', 'tokenized_comment',\n",
      "       'cleaned_comment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Đọc file CSV\n",
    "file_path = \"sentiments_v2_7001_7230.csv\"\n",
    "data = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "\n",
    "# In ra tên các cột để kiểm tra\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các cột trong DataFrame: Index(['userid', 'mtime', 'rating_star', 'comment', 'tokenized_comment',\n",
      "       'cleaned_comment'],\n",
      "      dtype='object')\n",
      "                                             comment  sentiment\n",
      "0             Giá rẻ. Mua xịt khử mùi ăn uóng khá ổn   0.700000\n",
      "1  thơm lắm mn oi có chai đào là khi xịt hòi là h...   0.086458\n",
      "2  eidighgghuđhuêd Dbddnd ĐĐ D D c chr ý nghĩa là...   0.000000\n",
      "3    Thơm nha các bạn nhưng k giữ đc mùi lâu đâu nhé  -0.050000\n",
      "4                         Sản phẩm giống hình xài ok   0.000000\n"
     ]
    }
   ],
   "source": [
    "# In ra tên các cột để kiểm tra\n",
    "print(\"Các cột trong DataFrame:\", data.columns)\n",
    "\n",
    "# Chỉ kiểm tra nếu cột 'comment' tồn tại\n",
    "if 'comment' in data.columns:\n",
    "    # Khởi tạo công cụ dịch\n",
    "    translator = Translator()\n",
    "\n",
    "    # Hàm dịch và phân tích cảm xúc\n",
    "    def analyze_sentiment(text):\n",
    "        # Dịch văn bản sang tiếng Anh\n",
    "        translated_text = translator.translate(text, src='vi', dest='en').text\n",
    "        # Phân tích cảm xúc bằng TextBlob\n",
    "        blob = TextBlob(translated_text)\n",
    "        return blob.sentiment.polarity  # Trả về điểm cảm xúc từ -1 (tiêu cực) đến 1 (tích cực)\n",
    "\n",
    "    # Áp dụng hàm cho 5 bình luận đầu tiên\n",
    "    data['sentiment'] = data['comment'].head(5).apply(analyze_sentiment)\n",
    "\n",
    "    # Hiển thị 5 bình luận đầu tiên với điểm cảm xúc\n",
    "    print(data[['comment', 'sentiment']].head(5))\n",
    "else:\n",
    "    print(\"Cột 'comment' không tồn tại trong DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các cột trong DataFrame: Index(['userid', 'mtime', 'rating_star', 'comment', 'tokenized_comment',\n",
      "       'cleaned_comment'],\n",
      "      dtype='object')\n",
      "                                             comment  \\\n",
      "0             Giá rẻ. Mua xịt khử mùi ăn uóng khá ổn   \n",
      "1  thơm lắm mn oi có chai đào là khi xịt hòi là h...   \n",
      "2  eidighgghuđhuêd Dbddnd ĐĐ D D c chr ý nghĩa là...   \n",
      "3    Thơm nha các bạn nhưng k giữ đc mùi lâu đâu nhé   \n",
      "4                         Sản phẩm giống hình xài ok   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0            Giá rẻ . Mua xịt khử mùi ăn uóng khá ổn  \n",
      "1  thơm lắm mn oi có chai đào là khi xịt hòi là h...  \n",
      "2  eidighgghuđhuêd Dbddnd ĐĐ D D c chr ý_nghĩa_là...  \n",
      "3    Thơm nha các bạn nhưng k giữ đc mùi lâu đâu nhé  \n",
      "4                         Sản_phẩm giống hình xài ok  \n"
     ]
    }
   ],
   "source": [
    "# Đọc file CSV và bỏ qua các dòng lỗi\n",
    "file_path = \"sentiments_v2_7001_7230.csv\"\n",
    "data = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "\n",
    "# In ra tên các cột để kiểm tra\n",
    "print(\"Các cột trong DataFrame:\", data.columns)\n",
    "\n",
    "# Giả sử cột chứa văn bản là 'text' (thay bằng tên cột tương ứng nếu khác)\n",
    "if 'comment' in data.columns:\n",
    "    # Tách từ cho từng dòng trong cột 'text' và lưu vào một cột mới 'tokenized_text'\n",
    "    data['tokenized_text'] = data['comment'].apply(ViTokenizer.tokenize)\n",
    "\n",
    "    output_file_path = \"sentiments_tk_7001_7230.csv\"\n",
    "    data.to_csv(output_file_path, index=False)  # Không lưu chỉ mục (index)\n",
    "    \n",
    "    print(data[['comment', 'tokenized_text']].head())\n",
    "else:\n",
    "    print(\"Không tìm thấy cột 'comment' trong DataFrame. Vui lòng kiểm tra lại tên cột.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các cột trong DataFrame: Index(['userid', 'mtime', 'rating_star', 'comment', 'tokenized_comment',\n",
      "       'cleaned_comment'],\n",
      "      dtype='object')\n",
      "Dữ liệu đã được lưu vào file: sentiments_v2_7001_7230.csv\n"
     ]
    }
   ],
   "source": [
    "# Danh sách các stop words tiếng Việt (có thể mở rộng thêm)\n",
    "stop_words = set([\n",
    "    \"và\", \"của\", \"là\", \"đã\", \"đang\", \"nên\", \"nhưng\", \"với\", \"bằng\", \"theo\", \n",
    "    \"làm\", \"từ\", \"trong\", \"lúc\", \"mà\", \"này\", \"có\", \"chưa\", \"vì\", \"sẽ\", \"khi\",\n",
    "    \"được\", \"không\", \"tôi\", \"bạn\", \"chúng\", \"mình\", \"một\", \"nhiều\", \"vài\", \n",
    "    \"cũng\", \"nào\", \"nếu\", \"hay\", \"kể\", \"vừa\", \"nên\", \"có thể\", \"làm\", \"mà\"\n",
    "])\n",
    "\n",
    "# In ra tên các cột để kiểm tra\n",
    "print(\"Các cột trong DataFrame:\", data.columns)\n",
    "\n",
    "# Giả sử cột chứa văn bản là 'comment' (thay bằng tên cột khác nếu cần)\n",
    "if 'comment' in data.columns:\n",
    "    # Tách từ trong cột 'comment'\n",
    "    data['tokenized_comment'] = data['comment'].apply(ViTokenizer.tokenize)\n",
    "    \n",
    "    # Bước loại bỏ stop words\n",
    "    def remove_stopwords(text):\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        return \" \".join(filtered_words)\n",
    "    \n",
    "    # Áp dụng loại bỏ stop words\n",
    "    data['cleaned_comment'] = data['tokenized_comment'].apply(remove_stopwords)\n",
    "\n",
    "    # Lưu kết quả vào file CSV mới\n",
    "    output_file_path = \"sentiments_v2_7001_7230.csv\"\n",
    "    data.to_csv(output_file_path, index=False)  # Không lưu chỉ mục (index)\n",
    "\n",
    "    print(f\"Dữ liệu đã được lưu vào file: {output_file_path}\")\n",
    "else:\n",
    "    print(\"Không tìm thấy cột 'comment' trong DataFrame. Vui lòng kiểm tra lại tên cột.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematization đã hoàn thành và lưu vào file lemmatized_sentiments.csv\n"
     ]
    }
   ],
   "source": [
    "# Đặt đường dẫn tới mô hình VnCoreNLP\n",
    "vncorenlp_path = r'D:\\Y4 HK1\\VnCoreNLP\\VnCoreNLP-1.1.1.jar'  # Thay bằng đường dẫn chính xác của bạn\n",
    "\n",
    "# Khởi tạo VnCoreNLP\n",
    "vncorenlp = VnCoreNLP(vncorenlp_path)\n",
    "\n",
    "if 'comment' in data.columns:\n",
    "    # Áp dụng lemmatization cho mỗi dòng trong cột 'comment'\n",
    "    def lemmatize_text(text):\n",
    "    # Dùng VnCoreNLP để annotate văn bản\n",
    "        annotated = vncorenlp.annotate(text)\n",
    "        \n",
    "        # Trích xuất các từ đã được lemmatize từ dữ liệu trả về\n",
    "        words = []\n",
    "        for sentence in annotated.get('sentences', []):\n",
    "            for token in sentence:\n",
    "                # Kiểm tra nếu 'lemma' tồn tại trong token\n",
    "                if 'lemma' in token:\n",
    "                    words.append(token['lemma'])\n",
    "                else:\n",
    "                    words.append(token['form'])  # Nếu không có 'lemma', lấy từ gốc\n",
    "        \n",
    "        # Trả về các từ đã lemmatized\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Áp dụng lemmatization cho mỗi dòng trong cột 'comment'\n",
    "    data['lemmatized_comment'] = data['comment'].apply(lemmatize_text)\n",
    "\n",
    "    # Lưu lại kết quả vào file CSV mới\n",
    "    data.to_csv('lemmatized_sentiments.csv', index=False)  # Lưu vào file mới\n",
    "\n",
    "    print(\"Lematization đã hoàn thành và lưu vào file lemmatized_sentiments.csv\")\n",
    "else:\n",
    "    print(\"Không tìm thấy cột 'comment' trong DataFrame.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
